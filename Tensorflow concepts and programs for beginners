'''
Tensor flow is a google open source deep learning library for implementing various complex deep learning algorithms in few lines of code.
This is based on the concepts of ANN (Artificial neural networks). The theory for Neural networks will be uploaded soon.
Below is the quick start guide for begineers to start with tensorflow programming.
'''

'''
Tensorflow programming principles:
1) Import the Tensorflow library
2) Get/Import the data
3) Set number of nodes for each layer
4) Set number of cycles for a neural network
5) Set batch size
6) Create layers (hidden + output) which is a combination of weights and bias
7) Create flow of Neural network.
8) Creating function to train neural networks
9) Create a session to run the tensorflow graph generated by the program.
10) Find the testing accuracy
'''

'''
Execute each program to see how tensorflow library is used. 
The details of tensorflow libraries can be found on tensorflow official page.
https://www.tensorflow.org/
'''


################################################### prog 1 ################################################
import tensorflow as tf 
# declaring 2 variables
x1 = tf.constant(10)
x2 = tf.constant(2)
# using tf.mul to multiple and storing in tensorflow graph .. no computation here only creating graph.
op= tf.mul(x1,x2)

'''
# creating a session to run the op 
sess= tf.Session()
# final o/p is stored in result variable after running the session. Here computation happened.
result=(sess.run(op))
# printing the output
print(result)
sess.close()  
'''
with tf.Session() as sess:
	print(sess.run(op))

################################################### prog 2 ################################################

import tensorflow as tf
v1= tf.constant('hello world using tensorflow!')
print(v1)
sess=tf.Session()
print('Without byte string\n:',sess.run(v1).decode())
print('With byte string(the letter b in the starting):\n',sess.run(v1))

################################################### prog 3 ################################################
''' in order to create a neural network we perform the following things:
1) Inputs with weights are given to layer 1 and it fires with the help of activation function.
2) The output of layer 1 acts as an input to layer 2 and again it fires with the help of activation function.
3) The final o/p is compared to actual or expected o/p by the help of a function called cost function. (cross entropy)
4) In order to minimize the cost function we use optimization techniques using optimizer.(eg: AdamOptimizer,SGD..etc)
5) Then we use backpropogation to adjust the weights based on the o/p
6) A cycle continues to reduce the error and increase accuracy which is called as EPOCH. (feed forward + backpropogation)
'''

import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("/tmp/data/", one_hot=True)
''' 
The one hot parameter means that under one situation or condition 1 pixel would be on and rest would be off
0 = [1,0,0,0,0,0,0,0,0]
1 = [0,1,0,0,0,0,0,0,0]
2 = [0,0,1,0,0,0,0,0,0]
3 = [0,0,0,1,0,0,0,0,0]
'''
# creating nodes for 3 layes :

n_nodes_l1=400
n_nodes_l2=400
n_nodes_l3=400

#the system will take 100 images in a batch and will process
batch_size =100

# number of classes i.e need to identify the images from 0 to 9 so 10 classes.
n_classes=10

# placeholder 
# here the images which we are going to process is of 28x28 pixels
x= tf.placeholder('float')
y= tf.placeholder('float')

# creating a neural network model.

# we always use [(input data*weight) + bias] for each layer. The use of bias is in case of i/p*weight = 0 so to trigger the o/p using activation function.
def nn_model(data):
	# defining the properties of hidden layer 1 using weight and bias
	hidden_l1={'weight': tf.Variable(tf.random_normal([784,n_nodes_l1])),
				 'bias': tf.Variable(tf.random_normal([n_nodes_l1]))
				 }	

	hidden_l2={'weight': tf.Variable(tf.random_normal([n_nodes_l1,n_nodes_l2])),
				 'bias': tf.Variable(tf.random_normal([n_nodes_l2]))
				 }	

	hidden_l3={'weight': tf.Variable(tf.random_normal([n_nodes_l2,n_nodes_l3])),
				 'bias': tf.Variable(tf.random_normal([n_nodes_l3]))
				 }	

	output_l1={'weight': tf.Variable(tf.random_normal([n_nodes_l3,n_classes])),
				 'bias': tf.Variable(tf.random_normal([n_classes]))
				 }	

 	# creating the flow of data (input_data*weight)+bias

	l1=tf.add(tf.matmul(data,hidden_l1['weight']),hidden_l1['bias'])
 	# now passing through activation function.
	l1=tf.nn.relu(l1)

	l2= tf.add(tf.matmul(l1,hidden_l2['weight']),hidden_l2['bias'])
 	# now passing through activation function.
	l2=tf.nn.relu(l2)

	l3= tf.add(tf.matmul(l2,hidden_l3['weight']),hidden_l3['bias'])
 	# now passing through activation function.
	l3=tf.nn.relu(l3)

	output = tf.matmul(l3,output_l1['weight'])+ output_l1['bias']

	return output

	# creating a function to train neural network model:

def train_nn_model(x):

		prediction = nn_model(x)
		# the below cost function calculates the difference between the prediction we got and the known value of y.
		cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(prediction,y))
		# Inorder to reduce the cost function we'll be using the optimizer. 
		# The below optimizer is based on gradiant decent concept.The adam optimizer has parameter learning rate and default is 0.001
		optimizer= tf.train.AdamOptimizer().minimize(cost)
		# this is combination of feed forward and back propogation so total movement across nn would be 20
		nn_epochs=10

		# creating a session to run the model
		with tf.Session() as sess:
			sess.run(tf.initialize_all_variables())

			# the below code is training the model and optimizing the weights to reduce the error in every cycle.
			for i in range(nn_epochs):
				epoch_loss=0 # variable to calculate the loss after each cycle, initialize with 0
				# for batch 1 it will take 550 images to train. mnist.train.num_examples=55000
				for j in range(int(mnist.train.num_examples/batch_size)) :
					epx,epy=mnist.train.next_batch(batch_size)
					j,c=sess.run([optimizer,cost],feed_dict={x:epx,y:epy})
					epoch_loss+=c
					print(i,' epoch completed out of ',nn_epochs, ' and loss is ',epoch_loss)

				# this will return the index of maximum values in these arrays.
				# it means one hot and rest 0 based on the pixels and will tell that both are identical.
				correct = tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))
				# calculating the accuracy:
				accuracy = tf.reduce_mean(tf.cast(correct,'float'))
				# the below code is performing the testing accuracy of model.
				print('Accuracy of neural net: ', accuracy.eval({x:mnist.test.images,y:mnist.test.labels}))

train_nn_model(x)

################################################### prog 4 ################################################
'''
The below program converts the input file into tensors and further can be used for sentiment analysis.
Converting the text data into tensors for tensorflow Neural network to process:

1) All the words must be assigned some number which will represent in the form of arrays so that it can be converted into tensors for tensorflow to process.

2) We'll be creating a lexicon i.e dictionary of words.

3) Ex: if our lexicon is [cat,dog,chair] and 
our sentence is : The cat and dog are hiding behing the chair.
In this case the vector for the above sentence is 
[1,1,1] because all the worlds from [cat,dog,chair] is present in the sentence 
"The cat and dog are hiding behing the chair"

Another example :

if our lexicon is [car,man,women,spaceship]
our sentence is : A man is driving a car.
vector representation of the above sentence using lexicon:
[1,1,0,0] because car and man are present in the sentence.
'''

import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import numpy as np 
import random
import pickle
from collections import Counter

lemmatizer= WordNetLemmatizer()
no_of_lines= 100000

def create_lexicon(pos,neg):
	lexicon =[]

	# loop going through pos and neg keywords representing type of data
	for file in [pos,neg]:
		with open(file,'r') as f:
			contents= f.readlines() # reading the file and placing the content in a single line which is stored in a list
			for l in contents[:no_of_lines]:
				token_words= word_tokenize(l.lower())
				lexicon+=token_words

	'''need to lemmitize the words which are now present in lexicon 
	(need only the words which don't have ing etc.. ex: eat-ing so need eat)'''
	lexicon= [lemmatizer.lemmatize(i) for i in lexicon]
	
	# counter will count each word present in lexicon and will return dictionary ex: {I:232, hello:442 , end: 342, eat: 863}
	word_count= Counter(lexicon)

	'''creating final lexicon in which we will keep only those words which are no so frequent because chances are that if we don't 
	exclude the frequent words we'll end up including words in lexicon like 'is','was','i','am' whose meanings are not important'''

	final_lexicon=[]
	for i in word_count:
		if 7000>word_count[i]>20: # this included the words whose occurance is less than 7000 but more than 20 in lexicon.
			final_lexicon.append(i)
	return final_lexicon

# the below function is used to create features of data based in the form of arrays.
def feature_creation(sample,lexicon,classification):
	featureset=[]

	# reading and storing the content of sample passed in this function
	with open(sample,'r') as f:
		content= f.readlines()

	# the below code is used to perform tokenization and lemmitization on the words present in content.
		for j in content[:no_of_lines]:
			current_words= word_tokenize(j.lower())
			current_words =[lemmatizer.lemmatize(i) for i in current_words]
			# for text analysis the number of features are actually the no. of lexicons.
			features = np.zeros(len(lexicon))

		# logic to check the words in lexicon , if yes then get the index of it and assign 1 on that position.

		for w in current_words:
			if w.lower() in lexicon:
				index_value= lexicon.index(w.lower())
				features[index_value]+=1
		features= list(features)
		featuresets=[]
		# creating a list by the combination of features and classification 
		#( for +ive classification will be [1,0] for -ive will be [0,1])
		featuresets.append([features,classification])

	return featuresets

# The below function is used to return the training and testing variables based on the lexicons and features.
def train_test_func(pos,neg,test_size=0.01):
	lexicon= create_lexicon('pos.txt','neg.txt')
	features = []
	features+=feature_creation('pos.txt',lexicon,[1,0])
	features+=feature_creation('neg.txt',lexicon,[0,1])

	# converting features into numpy array
	features=np.array(features)

	# creating the testing size 
	testing_size = int(test_size*len(features))

	# creating training values:
	train_x = list (features[:,0][:-testing_size])
	train_y = list (features[:,1][:-testing_size])

	# creating testing values:
	test_x = list (features[:,0][-testing_size:])
	test_y = list (features[:,1][-testing_size:])

	return train_x,train_y,test_x,test_y

if __name__=="__main__":
	train_x,train_y,test_x,test_y=train_test_func('pos.txt','neg.txt')
	with open ('sentiment_pickle.pickle','wb') as f:
		pickle.dump([train_x,train_y,test_x,test_y],f)
		


















