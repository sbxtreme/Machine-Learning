'''
Tensor flow is a google open source deep learning library for implementing various complex deep learning algorithms in few line of code.
This is based on the concepts of ANN (Artificial neural networks). The theory for Neural networks will be uploaded soon.
Below is the quick start guide for begineers to start with tensorflow programming.
'''

'''
Tensorflow principles:
1) Import the Tensorflow library
2) Get/Import the data
3) Set number of nodes for each layer
4) Set number of cycles for a neural network
5) Set batch size
6) Create layers (hidden + output) which is a combination of weights and bias
7) Create flow of Neural network.
8) Creating function to train neural networks
'''

'''
Execute each program to see how tensorflow library is used. 
The details of tensorflow libraries can be found on tensorflow official page.
https://www.tensorflow.org/
'''


################################################### prog 1 ################################################
import tensorflow as tf 
# declaring 2 variables
x1 = tf.constant(10)
x2 = tf.constant(2)
# using tf.mul to multiple and storing in tensorflow graph .. no computation here only creating graph.
op= tf.mul(x1,x2)

'''
# creating a session to run the op 
sess= tf.Session()
# final o/p is stored in result variable after running the session. Here computation happened.
result=(sess.run(op))
# printing the output
print(result)
sess.close()  
'''
with tf.Session() as sess:
	print(sess.run(op))

################################################### prog 2 ################################################

import tensorflow as tf
v1= tf.constant('hello world using tensorflow!')
print(v1)
sess=tf.Session()
print('Without byte string\n:',sess.run(v1).decode())
print('With byte string(the letter b in the starting):\n',sess.run(v1))

################################################### prog 3 ################################################
''' in order to create a neural network we perform the following things:
1) Inputs with weights are given to layer 1 and it fires with the help of activation function.
2) The output of layer 1 acts as an input to layer 2 and again it fires with the help of activation function.
3) The final o/p is compared to actual or expected o/p by the help of a function called cost function. (cross entropy)
4) In order to minimize the cost function we use optimization techniques using optimizer.(eg: AdamOptimizer,SGD..etc)
5) Then we use backpropogation to adjust the weights based on the o/p
6) A cycle continues to reduce the error and increase accuracy which is called as EPOCH. (feed forward + backpropogation)
'''
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("/tmp/data/", one_hot=True)
''' 
The one hot parameter means that under one situation or condition 1 pixel would be on and rest would be off
0 = [1,0,0,0,0,0,0,0,0]
1 = [0,1,0,0,0,0,0,0,0]
2 = [0,0,1,0,0,0,0,0,0]
3 = [0,0,0,1,0,0,0,0,0]
'''
# creating nodes for 3 layes :

n_nodes_l1=500
n_nodes_l2=500
n_nodes_l3=500

#the system will take 100 images in a batch and will process
batch_size =100

# number of classes i.e need to identify the images from 0 to 9 so 10 classes.
n_classes=10

# placeholder 
# here the images which we are going to process is of 28x28 pixels
x= tf.placeholder('float')
y= tf.placeholder('float')

# creating a neural network model.

# we always use [(input data*weight) + bias] for each layer. The use of bias is in case of i/p*weight = 0 so to trigger the o/p using activation function.
def nn_model(data):
	# defining the properties of hidden layer 1 using weight and bias
	hidden_l1={'weight': tf.Variable(tf.random.normal([784,n_nodes_l1])),
				 'bias': tf.Variable(tf.random.normal([n_nodes_l1]))
				 }	

	hidden_l2={'weight': tf.Variable(tf.random.normal([n_nodes_l1,n_nodes_l2])),
				 'bias': tf.Variable(tf.random.normal([n_nodes_l2]))
				 }	

	hidden_l3={'weight': tf.Variable(tf.random.normal([n_nodes_l2,n_nodes_l3])),
				 'bias': tf.Variable(tf.random.normal([n_nodes_l3]))
				 }	

	output_l1={'weight': tf.Variable(tf.random.normal([n_nodes_l3,n_classes])),
				 'bias': tf.Variable(tf.random.normal([n_classes]))
				 }	

 	# creating the flow of data (input_data*weight)+bias

	l1=tf.add(tf.matmul(data,hidden_l1['weight'])+hidden_l1['bias'])
 	# now passing through activation function.
	l1=tf.nn.relu(l1)

	l2= tf.add(tf.matmul(l1,hidden_l2['weight'])+hidden_l2['bias'])
 	# now passing through activation function.
	l2=tf.nn.relu(l2)

	l3= tf.add(tf.matmul(l2,hidden_l3['weight'])+hidden_l3['bias'])
 	# now passing through activation function.
	l3=tf.nn.relu(l3)

	output = tf.matmul(l3,output_l1['weight'])+ output_l1['bias']

	return output
