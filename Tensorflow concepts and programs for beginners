'''
Tensor flow is a google open source deep learning library for implementing various complex deep learning algorithms in few line of code.
This is based on the concepts of ANN (Artificial neural networks). The theory for Neural networks will be uploaded soon.
Below is the quick start guide for begineers to start with tensorflow programming.
'''

'''
Tensorflow programming principles:
1) Import the Tensorflow library
2) Get/Import the data
3) Set number of nodes for each layer
4) Set number of cycles for a neural network
5) Set batch size
6) Create layers (hidden + output) which is a combination of weights and bias
7) Create flow of Neural network.
8) Creating function to train neural networks
9) Create a session to run the tensorflow graph generated by the program.
10) Find the testing accuracy
'''

'''
Execute each program to see how tensorflow library is used. 
The details of tensorflow libraries can be found on tensorflow official page.
https://www.tensorflow.org/
'''


################################################### prog 1 ################################################
import tensorflow as tf 
# declaring 2 variables
x1 = tf.constant(10)
x2 = tf.constant(2)
# using tf.mul to multiple and storing in tensorflow graph .. no computation here only creating graph.
op= tf.mul(x1,x2)

'''
# creating a session to run the op 
sess= tf.Session()
# final o/p is stored in result variable after running the session. Here computation happened.
result=(sess.run(op))
# printing the output
print(result)
sess.close()  
'''
with tf.Session() as sess:
	print(sess.run(op))

################################################### prog 2 ################################################

import tensorflow as tf
v1= tf.constant('hello world using tensorflow!')
print(v1)
sess=tf.Session()
print('Without byte string\n:',sess.run(v1).decode())
print('With byte string(the letter b in the starting):\n',sess.run(v1))

################################################### prog 3 ################################################
''' in order to create a neural network we perform the following things:
1) Inputs with weights are given to layer 1 and it fires with the help of activation function.
2) The output of layer 1 acts as an input to layer 2 and again it fires with the help of activation function.
3) The final o/p is compared to actual or expected o/p by the help of a function called cost function. (cross entropy)
4) In order to minimize the cost function we use optimization techniques using optimizer.(eg: AdamOptimizer,SGD..etc)
5) Then we use backpropogation to adjust the weights based on the o/p
6) A cycle continues to reduce the error and increase accuracy which is called as EPOCH. (feed forward + backpropogation)
'''

import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("/tmp/data/", one_hot=True)
''' 
The one hot parameter means that under one situation or condition 1 pixel would be on and rest would be off
0 = [1,0,0,0,0,0,0,0,0]
1 = [0,1,0,0,0,0,0,0,0]
2 = [0,0,1,0,0,0,0,0,0]
3 = [0,0,0,1,0,0,0,0,0]
'''
# creating nodes for 3 layes :

n_nodes_l1=400
n_nodes_l2=400
n_nodes_l3=400

#the system will take 100 images in a batch and will process
batch_size =100

# number of classes i.e need to identify the images from 0 to 9 so 10 classes.
n_classes=10

# placeholder 
# here the images which we are going to process is of 28x28 pixels
x= tf.placeholder('float')
y= tf.placeholder('float')

# creating a neural network model.

# we always use [(input data*weight) + bias] for each layer. The use of bias is in case of i/p*weight = 0 so to trigger the o/p using activation function.
def nn_model(data):
	# defining the properties of hidden layer 1 using weight and bias
	hidden_l1={'weight': tf.Variable(tf.random_normal([784,n_nodes_l1])),
				 'bias': tf.Variable(tf.random_normal([n_nodes_l1]))
				 }	

	hidden_l2={'weight': tf.Variable(tf.random_normal([n_nodes_l1,n_nodes_l2])),
				 'bias': tf.Variable(tf.random_normal([n_nodes_l2]))
				 }	

	hidden_l3={'weight': tf.Variable(tf.random_normal([n_nodes_l2,n_nodes_l3])),
				 'bias': tf.Variable(tf.random_normal([n_nodes_l3]))
				 }	

	output_l1={'weight': tf.Variable(tf.random_normal([n_nodes_l3,n_classes])),
				 'bias': tf.Variable(tf.random_normal([n_classes]))
				 }	

 	# creating the flow of data (input_data*weight)+bias

	l1=tf.add(tf.matmul(data,hidden_l1['weight']),hidden_l1['bias'])
 	# now passing through activation function.
	l1=tf.nn.relu(l1)

	l2= tf.add(tf.matmul(l1,hidden_l2['weight']),hidden_l2['bias'])
 	# now passing through activation function.
	l2=tf.nn.relu(l2)

	l3= tf.add(tf.matmul(l2,hidden_l3['weight']),hidden_l3['bias'])
 	# now passing through activation function.
	l3=tf.nn.relu(l3)

	output = tf.matmul(l3,output_l1['weight'])+ output_l1['bias']

	return output

	# creating a function to train neural network model:

def train_nn_model(x):

		prediction = nn_model(x)
		# the below cost function calculates the difference between the prediction we got and the known value of y.
		cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(prediction,y))
		# Inorder to reduce the cost function we'll be using the optimizer. 
		# The below optimizer is based on gradiant decent concept.The adam optimizer has parameter learning rate and default is 0.001
		optimizer= tf.train.AdamOptimizer().minimize(cost)
		# this is combination of feed forward and back propogation so total movement across nn would be 20
		nn_epochs=10

		# creating a session to run the model
		with tf.Session() as sess:
			sess.run(tf.initialize_all_variables())

			# the below code is training the model and optimizing the weights to reduce the error in every cycle.
			for i in range(nn_epochs):
				epoch_loss=0 # variable to calculate the loss after each cycle, initialize with 0
				# for batch 1 it will take 550 images to train. mnist.train.num_examples=55000
				for j in range(int(mnist.train.num_examples/batch_size)) :
					epx,epy=mnist.train.next_batch(batch_size)
					j,c=sess.run([optimizer,cost],feed_dict={x:epx,y:epy})
					epoch_loss+=c
					print(i,' epoch completed out of ',nn_epochs, ' and loss is ',epoch_loss)

				# this will return the index of maximum values in these arrays.
				# it means one hot and rest 0 based on the pixels and will tell that both are identical.
				correct = tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))
				# calculating the accuracy:
				accuracy = tf.reduce_mean(tf.cast(correct,'float'))
				# the below code is performing the testing accuracy of model.
				print('Accuracy of neural net: ', accuracy.eval({x:mnist.test.images,y:mnist.test.labels}))

train_nn_model(x)
